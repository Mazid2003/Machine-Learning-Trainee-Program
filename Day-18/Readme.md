# ğŸ“… Day 18 â€“ Optimization Techniques in Deep Learning

**Date:01/07/2025**

**ğŸ¯ Learning Objectives**

By the end of this notebook or document, you will:

- âœ… Understand optimization in machine learning.
- âœ… Learn about loss functions and their role.
- âœ… Explore basic to advanced optimization algorithms (SGD, Momentum, RMSProp, Adam).
- âœ… Grasp challenges like local minima, saddle points, and vanishing gradients.
- âœ… Understand adaptive learning rate methods.
- âœ… Learn how to apply learning rate schedules, early stopping, and initialization techniques.

## 1ï¸âƒ£ What is Optimization in ML?

ğŸ‘‰ Optimization is the process of adjusting model parameters (weights & biases) to minimize the loss function.

The goal is to find:

**Î¸âˆ—=arg Î¸minâ€‹ L(Î¸)**

where:

Î¸ = model parameters

L(Î¸) = loss function

**âœ… Outcome: Better predictions on training and unseen data.**

## 2ï¸âƒ£ Role of the Loss Function

The loss function quantifies how well a model performs.

**Common examples:**

**MSE (Mean Squared Error)** â†’ for regression

**Cross-Entropy Loss** â†’ for classification

ğŸ‘‰ The optimizer works to minimize this loss.

## 3ï¸âƒ£ Gradient Descent: The Foundation

Gradient Descent updates parameters in the opposite direction of the gradient:

**Î¸:=Î¸âˆ’Î·âˆ‡Î¸L(Î¸)**

where:

ğœ‚ = learning rate

âˆ‡ğœƒğ¿(ğœƒ)= gradient of loss w.r.t parameters

| Type                    | Description             | Pros         | Cons               |
| ----------------------- | ----------------------- | ------------ | ------------------ |
| **Batch GD**            | Uses all data each step | Stable       | Slow for big data  |
| **Stochastic GD (SGD)** | 1 sample at a time      | Fast updates | Noisy, less stable |
| **Mini-batch GD**       | Small batches (32, 64)  | Fast, stable | Standard in DL     |

## 4ï¸âƒ£ Learning Rate: Critical Hyperparameter

**Too large** â†’ divergence

**Too small**â†’ slow convergence

**âœ… Learning rate schedules or adaptive methods help!**

## 5ï¸âƒ£ Challenges in Optimization

| Challenge                     | Description                       | Example Fix                 |
| ----------------------------- | --------------------------------- | --------------------------- |
| Local minima                  | Stuck in suboptimal valleys       | SGD noise, momentum         |
| Saddle points                 | Flat + curved directions          | Momentum, adaptive methods  |
| Vanishing/exploding gradients | Tiny/huge updates â†’ slow/unstable | Batch norm, good init, LSTM |

## 6ï¸âƒ£ Advanced Optimization Algorithms

| Optimizer | Key Idea                    | Advantage             | Drawback              |
| --------- | --------------------------- | --------------------- | --------------------- |
| SGD       | Vanilla GD                  | Simple                | Noisy, slow           |
| Momentum  | Adds velocity               | Faster, smoother      | Needs tuning Î³        |
| Nesterov  | Lookahead                   | Precise updates       | Slightly more complex |
| Adagrad   | Per-parameter LR            | Good for sparse data  | LR decays too much    |
| RMSProp   | Moving avg of squared grads | Stable LR             | Needs decay tuning    |
| Adam      | Combines momentum + RMSProp | Robust, fast, popular | Slight memory cost    |

## 7ï¸âƒ£ Mathematics

GD is an iterative scheme:

**ğœƒ(ğ‘¡+1)=ğœƒ(ğ‘¡)âˆ’ğœ‚âˆ‡ğœƒğ¿(ğœƒ^(ğ‘¡)**

**âœ… Convergence depends on:**

Smoothness of L

Learning rate

Convexity (rare in DL)

## 8ï¸âƒ£ Practical Tips

- âœ… Use good initialization (He for ReLU, Xavier for tanh).
- âœ… Normalize inputs.
- âœ… Use batch norm, dropout, early stopping.
- âœ… Use learning rate schedulers.

## 9ï¸âƒ£ Optimization in Deep Learning Frameworks

ğŸ‘‰ Example (Keras / TensorFlow):
```
from tensorflow.keras.optimizers import Adam, SGD
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

## ğŸ”Ÿ Summary Table

| Optimizer | Key Idea                    | Pros         | Cons               |
| --------- | --------------------------- | ------------ | ------------------ |
| SGD       | Basic GD                    | Simple       | Slow, noisy        |
| Momentum  | Adds velocity               | Faster       | Needs Î³            |
| NAG       | Lookahead                   | Accurate     | Slightly complex   |
| Adagrad   | Per-param LR                | Sparse data  | LR decay too much  |
| RMSProp   | Moving avg of grads         | Stable LR    | Needs decay tuning |
| Adam      | Combines Momentum + RMSProp | Fast, robust | Slight memory cost |

## 1ï¸âƒ£1ï¸âƒ£ Beyond Gradient-Based

- Genetic Algorithms

- Bayesian Optimization

- Second-Order Methods (Newton, quasi-Newton)

## 1ï¸âƒ£2ï¸âƒ£ Trends

- ğŸš€ New optimizers that generalize better
- ğŸš€ Adaptive methods with guarantees
- ğŸš€ Architecture-specific optimizers

## 1ï¸âƒ£3ï¸âƒ£ Practice Project

ğŸŸ£ Goal: Compare optimizers on CIFAR-10

ğŸŸ£ Plan:

**âœ… Implement models with:**

SGD

SGD + Momentum

RMSProp

Adam

**âœ… Visualize:**

Loss + accuracy curves

**âœ… Experiment:**

Learning rates (0.01, 0.001, etc)

Batch sizes (32, 64, 128)

**âœ… Apply:**

EarlyStopping

ReduceLROnPlateau

**âœ… Analyze:**

glorot_uniform vs he_normal init

ğŸ‘‰ Code Reference: [See my notebook link / repo file]

**ğŸ“Œ Example Code Snippet**
```
train_model(tf.keras.optimizers.Adam, lr=0.001, batch_size=64, initializer='he_normal')
```
ğŸ‘‰ Produces validation accuracy/loss curves + test accuracy.

### ğŸ“ˆ Visuals

ğŸ‘‰ Include:

Validation accuracy vs epoch for each optimizer

Loss vs epoch for each optimizer

Effect of LR / batch size in plots

Effect of init in plots










